{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "752d0693-3f35-4c0c-8ead-659168503847",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bertviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11241675-c07e-4985-9506-51e1337ec12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz import head_view\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "600f717e-eaf9-43c2-934d-298676b0f6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 17:37:15 [INFO] modeling_bert: Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "2023-04-05 17:37:15 [INFO] modeling_xlnet: Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering, BertConfig\n",
    "\n",
    "from pygaggle.rerank.base import Query, Text\n",
    "from pygaggle.rerank.transformer import EMBERT\n",
    "from ebert.embeddings import load_embedding, MappedEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c6ef5bf-9d29-4b78-b885-a1e82613dcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2496d49-6a82-46a6-ae87-6879de78d69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading regular BERT)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 17:37:23 [INFO] modeling_utils: loading configuration file output/monobert-large-msmarco-dbpedia_acc_batch_64_e6_annotated/monobert-large-msmarco-dbpedia_acc_batch_64_e6_fold1_annotated/config.json\n",
      "2023-04-05 17:37:23 [INFO] modeling_utils: Model config {\n",
      "  \"_name_or_path\": \"models/monobert-large-msmarco\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2023-04-05 17:37:23 [INFO] modeling_utils: loading weights file output/monobert-large-msmarco-dbpedia_acc_batch_64_e6_annotated/monobert-large-msmarco-dbpedia_acc_batch_64_e6_fold1_annotated/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EBERT embeddings\n",
      "MODELTYPE IS  monobert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 17:37:29 [INFO] modeling_utils: Weights from pretrained model not used in BertModel: ['bert.embeddings.position_ids']\n",
      "2023-04-05 17:37:29 [INFO] tokenization_utils: Model name 'output/monobert-large-msmarco-dbpedia_acc_batch_64_e6_annotated/monobert-large-msmarco-dbpedia_acc_batch_64_e6_fold1_annotated' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming 'output/monobert-large-msmarco-dbpedia_acc_batch_64_e6_annotated/monobert-large-msmarco-dbpedia_acc_batch_64_e6_fold1_annotated' is a path or url to a directory containing tokenizer files.\n",
      "2023-04-05 17:37:29 [INFO] tokenization_utils: loading file output/monobert-large-msmarco-dbpedia_acc_batch_64_e6_annotated/monobert-large-msmarco-dbpedia_acc_batch_64_e6_fold1_annotated/vocab.txt\n",
      "2023-04-05 17:37:29 [INFO] tokenization_utils: loading file output/monobert-large-msmarco-dbpedia_acc_batch_64_e6_annotated/monobert-large-msmarco-dbpedia_acc_batch_64_e6_fold1_annotated/added_tokens.json\n",
      "2023-04-05 17:37:29 [INFO] tokenization_utils: loading file output/monobert-large-msmarco-dbpedia_acc_batch_64_e6_annotated/monobert-large-msmarco-dbpedia_acc_batch_64_e6_fold1_annotated/special_tokens_map.json\n",
      "2023-04-05 17:37:29 [INFO] tokenization_utils: loading file output/monobert-large-msmarco-dbpedia_acc_batch_64_e6_annotated/monobert-large-msmarco-dbpedia_acc_batch_64_e6_fold1_annotated/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading  resources/wikipedia2vec/wikipedia-20190701/wikipedia2vec_500.pkl  and  mappers/wikipedia2vec-500-cased.monobert-base-cased.linear.npy\n"
     ]
    }
   ],
   "source": [
    "# replace <PATH-TO-SAVED-MODEL> with the real path of the saved model\n",
    "model_path = 'output/monobert-large-msmarco-dbpedia_acc_batch_64_e6_annotated/monobert-large-msmarco-dbpedia_acc_batch_64_e6_fold1_annotated'\n",
    "\n",
    "# load model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# load model\n",
    "\n",
    "model, model_emb = EMBERT.get_model(pretrained_model_name_or_path = model_path)\n",
    "reranker =  EMBERT(model = (model, model_emb))\n",
    "wiki_emb, mapper = reranker.ebert\n",
    "\n",
    "# reranker =  MonoBERT()\n",
    "# model, model_emb = reranker.get_model()\n",
    "# wiki_emb, mapper = reranker.ebert\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14975ff5-bc9f-4f71-a2a1-4db785cd7136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec208180-30e3-4f3e-b993-321037b43a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b45c68b-3732-465e-b210-842ca20328a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ret = reranker.tokenize_with_entities('Give me all movies directed by Francis Ford Coppola ENTITY/Francis_Ford_Coppola',\n",
    "#                                     'Francis Ford Coppola ENTITY/Francis_Ford_Coppola  (/ˈkoʊpələ/; born April 7, 1939) is an American ENTITY/Americans  film director, producer and screenwriter. He was part of the New Hollywood ENTITY/New_Hollywood  wave of filmmaking.After directing The Rain People ENTITY/The_Rain_People  in 1969, he won the Academy Award for Best Original Screenplay ENTITY/Academy_Award_for_Best_Original_Screenplay  as co-writer, with Edmund H. North ENTITY/Edmund_H._North , of Patton ENTITY/Patton_(film)  in 1970.')\n",
    "\n",
    "# ret = reranker.tokenize_with_entities('Give me all movies directed by Francis Ford Coppola',\n",
    "#                                       'Rumble Fish is an American 1983 drama film directed by Francis Ford Coppola.')\n",
    "\n",
    "# ret = reranker.tokenize_with_entities('Give me all movies directed by Francis Ford Coppola',\n",
    "#                                       'Rumble Fish is an American 1983 drama film directed by Francis Ford Coppola.')\n",
    "\n",
    "# input_text = ('Give me all movies directed by Francis Ford Coppola ENTITY/Francis_Ford_Coppola',\n",
    "#                                     'Rumble Fish ENTITY/Rumble_Fish  is an American ENTITY/United_States  1983 drama film directed by Francis Ford Coppola ENTITY/Francis_Ford_Coppola .')# It is based on the novel Rumble Fish ENTITY/Rumble_Fish  by S. E. Hinton ENTITY/S._E._Hinton , who also co-wrote the screenplay.The film centers on the relationship between Motorcycle Boy (Mickey Rourke ENTITY/Mickey_Rourke ), a revered former gang leader wishing to live a more peaceful life, and his younger brother, Rusty James ENTITY/Rumble_Fish  (Matt Dillon ENTITY/Matt_Dillon ), an uncool teenaged hoodlum who aspires to become as feared as Motorcycle Boy.')\n",
    "\n",
    "# Query = 'Show me all songs from Bruce Springsteen ENTITY/Bruce_Springsteen  released between 1980 and 1990.'\n",
    "# Passage = \"Point Blank ENTITY/Sean_Kingston  is a song written by Bruce Springsteen ENTITY/Bruce_Springsteen  and first released on Springsteen ENTITY/Bruce_Springsteen 's 1980 album The River ENTITY/The_River_(Bruce_Springsteen_song) .  In Europe ENTITY/Europe , it was also released as a single in 1981, backed by another song from The River ENTITY/The_River_(Bruce_Springsteen_song) , Ramrod ENTITY/Super_Troopers   Although it was not released as a single in the US ENTITY/Billboard_Hot_100 , it did reach #20 on the Billboard Mainstream Rock Tracks ENTITY/Mainstream_Rock_(chart)  chart.\"\n",
    "\n",
    "Query = 'Give me the capitals of all countries in Africa ENTITY/Africa'\n",
    "Passage = 'Dakar ENTITY/Dakar  is the capital and largest city of Senegal'# .It is located on the Cap-Vert Peninsula ENTITY/Cap-Vert  on the Atlantic coast ENTITY/East_Coast_of_the_United_States  and is the westernmost city in the Old World ENTITY/Europe  and on the African ENTITY/Africa  mainland.'\n",
    "\n",
    "# Query = 'Give me the capitals of all countries in Africa'\n",
    "# Passage = 'List of African  dependencies — including the respective capitals.'\n",
    "\n",
    "#Query = 'Give me all movies directed by Francis Ford Coppola ENTITY/Francis_Ford_Coppola'\n",
    "#Passage = 'Rumble Fish ENTITY/Rumble_Fish  is an American ENTITY/United_States  1983 drama film directed by Francis Ford Coppola ENTITY/Francis_Ford_Coppola. It is based on the novel Rumble Fish ENTITY/Rumble_Fish  by S. E. Hinton ENTITY/S._E._Hinton , who also co-wrote the screenplay.The film centers on the relationship between Motorcycle Boy (Mickey Rourke ENTITY/Mickey_Rourke ), a revered former gang leader wishing to live a more peaceful life, and his younger brother, Rusty James ENTITY/Rumble_Fish  (Matt Dillon ENTITY/Matt_Dillon ), an uncool teenaged hoodlum who aspires to become as feared as Motorcycle Boy.'\n",
    "\n",
    "#Passage = 'Christopher Coppola ENTITY/Christopher_Coppola  (born January 25, 1962) is a film director and producer.'\n",
    "\n",
    "\n",
    "# Query = 'Show me all songs from Bruce Springsteen released between 1980 and 1990.'\n",
    "# Passage = \"Bruce Springsteen 's Video Anthology / 1978–88 is a collection of 18 music videos made on his behalf, released in VHS format on January 31, 1989.It was reissued as The Complete Video Anthology / 1978-2000 by Sony in DVD on January 16, 2001, adding a second disc with 15 additional music videos or other clips.\"\n",
    "\n",
    "input_text = (Query, Passage)\n",
    "\n",
    "# input_text = ('Give me all movies directed by Francis Ford Coppola',\n",
    "#                                      'Christopher Coppola (born January 25, 1962) is a film director and producer')\n",
    "\n",
    "# input_text = ('Give me all movies directed by Francis Ford Coppola ENTITY/Francis_Ford_Coppola',\n",
    "#                                     'Christopher Coppola ENTITY/Christopher_Coppola  (born January 25, 1962) is a film director and producer.')\n",
    "\n",
    "\n",
    "#input_text = ('What is the capital of Tuvalu ENTITY/Tuvalu', 'It is Funafuti ENTITY/Funafuti')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08b55683-ba15-403e-85c5-8e0794e1c04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'give', 'me', 'the', 'capitals', 'of', 'all', 'countries', 'in', 'africa', '/', 'ENTITY/Africa', 'africa', '[SEP]', 'da', '##kar', '/', 'ENTITY/Dakar', 'da', '##kar', 'is', 'the', 'capital', 'and', 'largest', 'city', 'of', 'senegal', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "ret = reranker.tokenize_with_entities(*input_text, concat = True)\n",
    "\n",
    "\n",
    "#ret_ref = make_reference(ret['input_ids'])\n",
    "input_ids = reranker.vectorize(ret['input_ids'], model_emb, mapper, wiki_emb).to(device)\n",
    "#ref_input_ids = reranker.vectorize(ret_ref, model_emb, mapper, wiki_emb).to(device)\n",
    "token_type_ids =  ret['token_type_ids'].to(device)\n",
    "#ref_token_type_ids = ref_token_ids(token_type_ids).to(device)\n",
    "\n",
    "#ret['input_ids'], ret['token_type_ids']\n",
    "\n",
    "all_tokens = ret['input_ids']\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77bd2a96-0ec7-42ff-b210-6ce33e4fdd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'give', 'me', 'the', 'capitals', 'of', 'all', 'countries', 'in', 'africa', '/', 'ENTITY/Africa', '[SEP]', 'da', '##kar', '/', 'ENTITY/Dakar', 'is', 'the', 'capital', 'and', 'largest', 'city', 'of', 'senegal', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "ret = reranker.tokenize_with_entities(*input_text)\n",
    "\n",
    "#ret = reranker.tokenize_with_entities('What is the capital city of France', 'It is Paris')\n",
    "\n",
    "\n",
    "#ret_ref = make_reference(ret['input_ids'])\n",
    "input_ids = reranker.vectorize(ret['input_ids'], model_emb, mapper, wiki_emb).to(device)\n",
    "#ref_input_ids = reranker.vectorize(ret_ref, model_emb, mapper, wiki_emb).to(device)\n",
    "token_type_ids =  ret['token_type_ids'].to(device)\n",
    "#ref_token_type_ids = ref_token_ids(token_type_ids).to(device)\n",
    "\n",
    "#ret['input_ids'], ret['token_type_ids']\n",
    "\n",
    "all_tokens = ret['input_ids']\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eeef71-cff3-47c1-8a8b-f6f4fde70fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2328110d-1978-458f-af33-a69a33ca25e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aef7f7f-f3f3-4997-8361-8ffcf78a82e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac7cb905-d159-4373-a45e-b15d1521734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and retrieve attention\n",
    "#model_version = 'bert-base-uncased'\n",
    "#do_lower_case = True\n",
    "#model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
    "#tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)\n",
    "#sentence_a = \"The cat sat on the mat\"\n",
    "#sentence_b = \"The cat lay on the rug\"\n",
    "#inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt', add_special_tokens=True)\n",
    "#token_type_ids = inputs['token_type_ids']\n",
    "#input_ids = inputs['input_ids']\n",
    "attention = model(inputs_embeds = input_ids.float(), token_type_ids=token_type_ids.long())[-1]\n",
    "#input_id_list = input_ids[0].tolist() # Batch index 0\n",
    "#tokens = tokenizer.convert_ids_to_tokens(input_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04446113-a280-47c3-aa11-1cc88cd26483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a48deeb8-a69d-47de-a138-b1d121d3b73b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0771, -0.2276, -0.2115,  ..., -0.1406, -0.2277,  0.1183],\n",
       "          [-0.6424,  0.1819, -0.7668,  ...,  0.9381, -0.7975,  0.1105],\n",
       "          [ 0.5036, -0.5936,  1.0275,  ...,  0.5699,  1.2150,  0.4628],\n",
       "          ...,\n",
       "          [ 0.5882,  0.3434, -0.0348,  ..., -0.0466,  0.3253,  0.5850],\n",
       "          [ 1.0370, -0.2095, -0.1305,  ...,  0.4277,  0.3670, -0.0204],\n",
       "          [-0.2332,  0.2289, -0.0506,  ...,  0.2634,  0.3046,  0.2425]]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[-0.0551, -0.0922, -0.1385,  ...,  0.0046, -0.0835, -0.0605],\n",
       "          [-0.9527,  0.1549, -0.4586,  ...,  0.9570, -0.9576,  0.2355],\n",
       "          [ 0.5960, -0.9410,  0.9340,  ...,  0.5513,  1.7566,  0.3449],\n",
       "          ...,\n",
       "          [ 0.7757,  0.3955,  0.0337,  ..., -0.0783,  0.0973,  0.7050],\n",
       "          [ 1.0743,  0.0236,  0.1310,  ...,  0.2392,  0.4489, -0.1576],\n",
       "          [-0.1849,  0.2025, -0.1439,  ...,  0.2068,  0.3029,  0.3541]]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[-6.7219e-02, -1.1480e-01, -7.3194e-02,  ...,  7.8154e-04,\n",
       "           -4.4235e-02, -7.4520e-02],\n",
       "          [-6.6073e-01,  1.0382e-04, -4.4732e-02,  ...,  6.5188e-01,\n",
       "           -1.3090e+00,  5.2538e-01],\n",
       "          [ 1.3130e-01, -8.9556e-01,  9.2613e-01,  ...,  4.7800e-01,\n",
       "            1.7884e+00,  7.2356e-01],\n",
       "          ...,\n",
       "          [ 7.1071e-01,  2.1827e-01, -9.8827e-02,  ..., -6.5724e-02,\n",
       "            1.9478e-01,  3.7997e-01],\n",
       "          [ 1.0403e+00, -1.6847e-01,  3.2471e-01,  ...,  3.1515e-01,\n",
       "            7.1318e-01,  1.3991e-02],\n",
       "          [-1.8096e-01,  2.5629e-02, -1.9681e-01,  ...,  2.1658e-01,\n",
       "            2.8658e-01,  9.1819e-02]]], device='cuda:0',\n",
       "        grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[-5.8949e-02, -1.1899e-01, -1.0841e-01,  ...,  1.2862e-01,\n",
       "           -1.0546e-01, -7.5929e-02],\n",
       "          [-5.3481e-01,  5.5675e-02,  7.8095e-02,  ...,  7.3064e-01,\n",
       "           -1.2984e+00,  5.7800e-01],\n",
       "          [-5.1006e-02, -9.2933e-01,  6.3769e-01,  ...,  7.4765e-01,\n",
       "            1.3111e+00,  9.9439e-01],\n",
       "          ...,\n",
       "          [ 4.4446e-01,  3.9153e-01, -6.7901e-02,  ..., -9.3014e-02,\n",
       "            2.7066e-01,  2.2925e-01],\n",
       "          [ 9.6108e-01, -3.6915e-01,  2.4390e-01,  ...,  4.4288e-01,\n",
       "            5.0037e-01,  8.8026e-02],\n",
       "          [-2.0082e-01, -3.2335e-02, -2.1302e-01,  ...,  2.9163e-01,\n",
       "            2.2439e-01, -2.8770e-04]]], device='cuda:0',\n",
       "        grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[ 0.0700, -0.2081, -0.1523,  ...,  0.0960, -0.1640, -0.0518],\n",
       "          [-0.5245, -0.0713, -0.0369,  ...,  0.5629, -1.1090,  0.2535],\n",
       "          [-0.3327, -0.8016,  0.6780,  ...,  0.7361,  0.9507,  0.7873],\n",
       "          ...,\n",
       "          [ 0.6366,  0.1087, -0.1032,  ..., -0.1452,  0.1579,  0.1379],\n",
       "          [ 1.0089, -0.4706,  0.3631,  ...,  0.2719,  0.1749,  0.2635],\n",
       "          [-0.0405, -0.0677, -0.2321,  ...,  0.2721,  0.1305, -0.0766]]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[ 1.0051e-01, -1.4646e-01, -2.8590e-01,  ..., -5.4655e-02,\n",
       "           -1.8201e-01, -1.7215e-01],\n",
       "          [-3.7680e-01, -1.7897e-01, -1.2035e-01,  ...,  6.7570e-01,\n",
       "           -1.1484e+00,  6.3058e-01],\n",
       "          [-5.8873e-01, -1.0869e+00,  9.8024e-01,  ...,  6.0320e-01,\n",
       "            1.0415e+00,  5.0007e-01],\n",
       "          ...,\n",
       "          [ 5.8534e-01,  4.3827e-02, -1.3588e-01,  ..., -3.4341e-01,\n",
       "            3.7696e-01,  2.2550e-01],\n",
       "          [ 5.7954e-01, -4.9177e-01,  4.0705e-01,  ...,  3.5247e-01,\n",
       "           -6.8570e-02,  4.6609e-01],\n",
       "          [-1.3111e-04, -1.2726e-01, -1.8856e-01,  ...,  1.7750e-01,\n",
       "            1.1152e-01, -7.0702e-02]]], device='cuda:0',\n",
       "        grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[ 1.5699e-01, -1.1813e-01, -2.3140e-01,  ..., -1.0116e-01,\n",
       "           -4.0882e-01,  8.5319e-02],\n",
       "          [-7.9753e-02,  2.9545e-02, -3.4765e-01,  ...,  2.8370e-01,\n",
       "           -1.3365e+00,  7.1017e-01],\n",
       "          [-3.2197e-01, -1.0092e+00,  1.1858e+00,  ...,  7.4616e-01,\n",
       "            1.1701e+00,  6.1733e-01],\n",
       "          ...,\n",
       "          [ 5.2687e-01,  6.2883e-01,  3.0980e-01,  ..., -4.4567e-01,\n",
       "            2.6043e-01,  3.2196e-01],\n",
       "          [ 6.9711e-01, -3.6212e-01,  5.0978e-01,  ...,  4.4759e-01,\n",
       "           -2.0037e-01,  6.4823e-01],\n",
       "          [ 2.8476e-02, -1.2893e-01, -1.0932e-01,  ...,  1.2300e-01,\n",
       "            1.4112e-01,  1.2451e-03]]], device='cuda:0',\n",
       "        grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[ 0.1482,  0.0629, -0.5491,  ..., -0.3272, -0.5766,  0.1373],\n",
       "          [-0.3426,  0.1766, -0.8052,  ...,  0.4102, -1.6471,  0.6844],\n",
       "          [-0.5315, -1.3374,  1.3605,  ...,  0.7649,  1.1714,  0.7359],\n",
       "          ...,\n",
       "          [ 0.5134,  1.0315,  0.4059,  ..., -0.7037,  0.3314,  0.5097],\n",
       "          [ 0.4722, -0.3782,  0.4666,  ...,  0.5965, -0.5654,  0.9707],\n",
       "          [-0.0680, -0.0983, -0.2472,  ..., -0.0630,  0.2241,  0.0552]]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[-0.1819,  0.0850, -0.6537,  ..., -0.5249, -0.5149,  0.3654],\n",
       "          [-0.0308,  0.0707, -0.2968,  ...,  0.1046, -1.5300,  0.6215],\n",
       "          [-0.1801, -1.5513,  1.0418,  ...,  0.3312,  1.2828,  0.6592],\n",
       "          ...,\n",
       "          [ 0.5001,  0.7602,  0.3883,  ..., -0.5379,  0.2165,  0.4448],\n",
       "          [ 0.4624, -0.6419,  0.4438,  ...,  0.7691, -0.4936,  0.7849],\n",
       "          [-0.0729, -0.1586, -0.2216,  ..., -0.0625,  0.1455,  0.0235]]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[ 0.0790,  0.4272, -1.0462,  ..., -0.8534, -0.5202,  0.3958],\n",
       "          [ 0.0233,  0.2118, -0.4447,  ...,  0.4499, -1.6054,  0.4189],\n",
       "          [-0.0318, -1.2368,  0.7375,  ...,  1.1768,  1.1702,  1.0030],\n",
       "          ...,\n",
       "          [ 0.1399,  0.5554,  0.2860,  ..., -0.6000, -0.3731,  0.4993],\n",
       "          [ 0.1034, -0.4977, -0.1203,  ...,  0.7294, -0.3658,  0.4761],\n",
       "          [-0.0590, -0.2234, -0.1995,  ..., -0.0890,  0.1108,  0.0706]]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[ 0.0464,  0.4703, -1.1100,  ..., -0.7505, -0.3921,  0.3586],\n",
       "          [ 0.4421,  0.5601, -0.4639,  ...,  0.7666, -1.8204,  0.1221],\n",
       "          [ 0.1588, -1.1164,  0.7317,  ...,  1.0392,  1.0060,  0.5310],\n",
       "          ...,\n",
       "          [ 0.3250,  0.5124,  0.0041,  ..., -0.6986, -0.4911,  0.2996],\n",
       "          [ 0.4339, -0.7429, -0.3666,  ...,  0.6164, -0.3022,  0.4501],\n",
       "          [ 0.0024, -0.0613, -0.1712,  ..., -0.0919, -0.0046,  0.0307]]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[-0.2187,  0.6519, -1.3464,  ..., -0.7888, -0.3934,  0.4524],\n",
       "          [-0.0529,  0.7245,  0.1825,  ...,  0.7838, -1.9446,  0.0115],\n",
       "          [-0.1034, -1.1496,  0.5138,  ...,  1.6953,  0.8641,  0.3116],\n",
       "          ...,\n",
       "          [ 0.4595,  0.4517, -0.2360,  ..., -0.6059, -0.2389,  0.5848],\n",
       "          [ 0.3924, -0.6197, -0.3206,  ...,  0.5171, -0.4067,  0.6018],\n",
       "          [ 0.0086, -0.0043, -0.0493,  ..., -0.0776, -0.0489,  0.0417]]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[-2.3766e-01,  5.1381e-01, -1.2156e+00,  ..., -5.0738e-01,\n",
       "           -5.0487e-01,  4.8883e-01],\n",
       "          [ 2.7911e-01,  5.3287e-01,  5.2198e-01,  ...,  4.9841e-01,\n",
       "           -1.8395e+00,  1.7558e-01],\n",
       "          [-3.3037e-01, -1.1960e+00,  5.2269e-01,  ...,  1.5194e+00,\n",
       "            1.0172e+00,  4.0755e-01],\n",
       "          ...,\n",
       "          [ 2.0782e-02,  8.3068e-01, -1.8917e-01,  ..., -5.7642e-01,\n",
       "           -4.1036e-01,  7.6228e-01],\n",
       "          [ 5.2391e-01, -3.7291e-01, -1.2385e-01,  ...,  5.1759e-01,\n",
       "           -2.7619e-01,  6.2924e-01],\n",
       "          [-5.8237e-04, -3.1978e-03, -4.8749e-02,  ..., -5.6954e-02,\n",
       "           -4.7941e-02,  1.8414e-02]]], device='cuda:0',\n",
       "        grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[-0.4490,  0.7515, -1.3574,  ..., -0.1570, -0.4496,  0.5106],\n",
       "          [ 0.2732,  0.5993,  0.3475,  ...,  0.4135, -1.4717,  0.0906],\n",
       "          [-0.1445, -0.9223,  0.0047,  ...,  0.8772,  1.0581,  0.3063],\n",
       "          ...,\n",
       "          [-0.4485,  0.6292, -0.2298,  ..., -0.2893, -0.4210,  0.6284],\n",
       "          [ 0.3112, -0.2680,  0.1294,  ...,  0.4706, -0.3275,  0.5864],\n",
       "          [-0.0231, -0.0047, -0.0900,  ..., -0.0599, -0.0400, -0.0016]]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[-0.4360,  0.9632, -1.5737,  ..., -0.1404, -0.5457,  0.4994],\n",
       "          [-0.1923,  0.3460,  0.1853,  ...,  0.1176, -1.4564,  0.0190],\n",
       "          [-0.6609, -0.7664,  0.0248,  ...,  0.1344,  1.2291,  0.3600],\n",
       "          ...,\n",
       "          [-0.6037,  0.3603,  0.0448,  ..., -0.7870, -0.9744,  0.2662],\n",
       "          [ 0.2923, -0.0561, -0.1836,  ...,  0.2577, -0.3131,  0.5290],\n",
       "          [ 0.0066, -0.0199, -0.0748,  ..., -0.0759,  0.0075, -0.0093]]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[-0.0296,  0.7574, -1.4961,  ...,  0.0634, -0.2134,  0.8480],\n",
       "          [-0.3256,  0.4974,  0.7059,  ...,  0.0957, -0.7534, -0.5023],\n",
       "          [-0.9381, -0.5094,  0.6810,  ...,  0.3063,  0.6680,  0.3637],\n",
       "          ...,\n",
       "          [-0.7671,  0.4993,  0.4826,  ..., -0.6603, -1.2220,  0.2468],\n",
       "          [ 0.0712,  0.1380, -0.2407,  ...,  0.1742, -0.4160,  0.3326],\n",
       "          [-0.0083, -0.0635, -0.1061,  ..., -0.1052, -0.0292, -0.0247]]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[-0.4180,  0.1556, -1.7797,  ...,  0.7146, -0.1347,  0.2813],\n",
       "          [-0.5553,  0.1753,  0.1354,  ..., -0.4154, -0.8991, -0.7054],\n",
       "          [-0.6228, -0.4395,  0.8248,  ...,  0.5122,  0.6040,  0.2422],\n",
       "          ...,\n",
       "          [-0.8693,  0.5161,  0.2535,  ..., -0.8208, -1.1567,  0.2775],\n",
       "          [-0.3286,  0.6749, -0.2370,  ...,  0.0584, -0.4728,  0.2364],\n",
       "          [-0.0371, -0.1209, -0.0611,  ..., -0.1223, -0.0211, -0.0785]]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[-0.2824,  0.5498, -1.6900,  ...,  0.5689, -0.1196,  0.4713],\n",
       "          [-0.5721,  0.0202, -0.0041,  ..., -0.0655, -0.7505, -0.3196],\n",
       "          [-0.8753, -0.3355,  0.4772,  ...,  0.5019,  0.6464,  0.8404],\n",
       "          ...,\n",
       "          [-0.1595,  0.3376,  0.2025,  ..., -0.8305, -0.9683,  0.2832],\n",
       "          [-0.3304,  0.6971, -0.3485,  ..., -0.3218, -0.4461,  0.0999],\n",
       "          [ 0.0231, -0.1629, -0.0025,  ..., -0.0739, -0.0565,  0.0222]]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[-0.2683,  0.3803, -1.6403,  ...,  0.8453, -0.3272,  0.6723],\n",
       "          [-0.2774,  0.0776,  0.4481,  ...,  0.4977, -0.6867, -0.4966],\n",
       "          [-0.6768, -0.0936,  0.1314,  ...,  0.5604,  0.5451,  0.7151],\n",
       "          ...,\n",
       "          [-0.3389,  0.2802,  0.5730,  ..., -0.4948, -0.9293,  0.1785],\n",
       "          [-0.5290,  0.3382, -0.6751,  ..., -0.1557, -0.4839,  0.1368],\n",
       "          [ 0.0087, -0.0966, -0.0413,  ..., -0.0413, -0.0144,  0.0305]]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[-0.4966, -0.0553, -1.5582,  ...,  0.4667, -0.6331,  0.4734],\n",
       "          [-0.5051,  0.2648,  0.8722,  ...,  0.3298, -0.8287, -0.5127],\n",
       "          [-0.6579, -0.0040,  0.2058,  ...,  0.5533,  0.9081,  0.8184],\n",
       "          ...,\n",
       "          [-0.3146,  0.1365,  0.6307,  ..., -0.7643, -0.9928,  0.2579],\n",
       "          [-0.5869,  0.2930, -0.6968,  ..., -0.2613, -0.4646,  0.0346],\n",
       "          [-0.0170, -0.0066, -0.0453,  ..., -0.0396, -0.0051,  0.0039]]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[-0.3541,  0.0815, -1.2609,  ...,  0.2719, -0.8991,  0.6739],\n",
       "          [-0.3059,  0.3590,  1.3939,  ...,  0.4679, -0.5577, -0.5323],\n",
       "          [-0.0478,  0.0535,  0.7562,  ...,  0.8221,  1.0192,  0.5946],\n",
       "          ...,\n",
       "          [-0.3792, -0.0747,  0.7928,  ..., -0.4907, -1.5686,  0.4170],\n",
       "          [-0.5207,  0.5635, -0.8023,  ..., -0.0752, -0.8245,  0.4933],\n",
       "          [ 0.0048,  0.0030, -0.0343,  ...,  0.0122, -0.0158,  0.0119]]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[-0.3158, -0.9182, -1.1365,  ...,  0.0312, -1.7500,  0.6013],\n",
       "          [-0.7348,  0.2160,  0.9423,  ...,  0.8485, -0.1665, -0.6178],\n",
       "          [-0.2684, -0.3320,  0.4926,  ...,  1.0783,  1.1277,  0.4196],\n",
       "          ...,\n",
       "          [-0.1769, -0.5689,  0.6200,  ..., -0.1342, -1.5421,  0.5069],\n",
       "          [-0.5289,  0.5595, -1.0342,  ..., -0.0048, -0.9182,  0.4480],\n",
       "          [-0.0274,  0.0268, -0.0587,  ..., -0.0300, -0.0485, -0.0500]]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[ 0.1371, -1.6892, -1.4480,  ...,  0.2233, -1.8872,  0.5348],\n",
       "          [-0.6326,  0.4595,  0.5069,  ...,  0.5117,  0.0456, -0.2857],\n",
       "          [-0.0697, -0.3875,  0.1718,  ...,  1.0850,  1.0454,  0.2110],\n",
       "          ...,\n",
       "          [ 0.1756, -0.5753,  0.5245,  ..., -0.1177, -1.7867,  0.4509],\n",
       "          [-0.2546,  0.3937, -0.6530,  ...,  0.1416, -1.1771,  0.2324],\n",
       "          [ 0.0096, -0.0448,  0.0195,  ..., -0.0320, -0.0079, -0.0576]]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[ 0.1119, -1.4779, -2.2009,  ..., -0.4058, -1.4145,  0.1636],\n",
       "          [-0.8816,  0.2380,  0.1494,  ...,  0.3788, -0.1441, -0.5671],\n",
       "          [-0.0943, -0.4606, -0.1696,  ...,  0.8041,  1.0269, -0.1454],\n",
       "          ...,\n",
       "          [-0.1595, -0.4895,  0.3795,  ..., -0.2165, -1.6905,  0.1358],\n",
       "          [-0.1851,  0.2749, -0.4981,  ..., -0.1000, -0.9802,  0.2449],\n",
       "          [ 0.0050, -0.0149,  0.0137,  ...,  0.0067,  0.0039, -0.0244]]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward>),\n",
       " tensor([[[ 0.6914,  0.0163, -0.5916,  ..., -0.3286, -0.9593, -0.3431],\n",
       "          [-0.1523, -0.3845,  0.1619,  ..., -0.0170,  0.1313, -0.2539],\n",
       "          [ 0.3883, -0.4547,  0.2503,  ...,  0.0867,  0.0695, -0.2803],\n",
       "          ...,\n",
       "          [-0.1486, -0.7140,  0.1648,  ..., -0.7767, -0.8211, -0.0307],\n",
       "          [-0.1158, -0.3120, -0.4033,  ..., -0.6668, -0.5614,  0.2186],\n",
       "          [ 0.8877, -0.3043,  0.4238,  ...,  0.0543,  0.0872, -0.0721]]],\n",
       "        device='cuda:0', grad_fn=<NativeLayerNormBackward>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5af61257-e548-4539-a7ae-9bbc2e65fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sub_attention = [np.round(100*a.item()) for a in attention[0][0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "71df1b0b-e744-4006-b3ca-8d8112f4dd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "848dbde0-87ea-4c34-b7d5-e7a406717705",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(head_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a7c33253-8466-4fa5-870b-0aeb1568888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(zip(sub_attention, all_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b8c5acae-dbf8-46d7-bc59-61895d98629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from TextAttentionHeatmapVisualization import text_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "aa3adbb3-8be4-4166-8940-afd6e8046ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pathname = 'text_attention_latex/africa.tex'\n",
    "#text_attention.generate(all_tokens, sub_attention, pathname, color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca5a7f51-10c0-428d-845d-595a2522a7d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The attention tensor does not have the correct number of dimensions. Make sure you set output_attentions=True when initializing your model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2458609/2821973555.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhead_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/bertviz/head_view.py\u001b[0m in \u001b[0;36mhead_view\u001b[0;34m(attention, tokens, sentence_b_start, prettify_tokens, layer, heads, encoder_attention, decoder_attention, cross_attention, encoder_tokens, decoder_tokens, include_layers)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minclude_layers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0minclude_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msentence_b_start\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             attn_data.append(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/bertviz/util.py\u001b[0m in \u001b[0;36mformat_attention\u001b[0;34m(attention, layers, heads)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# 1 x num_heads x seq_len x seq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_attention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             raise ValueError(\"The attention tensor does not have the correct number of dimensions. Make sure you set \"\n\u001b[0m\u001b[1;32m     12\u001b[0m                              \"output_attentions=True when initializing your model.\")\n\u001b[1;32m     13\u001b[0m         \u001b[0mlayer_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_attention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The attention tensor does not have the correct number of dimensions. Make sure you set output_attentions=True when initializing your model."
     ]
    }
   ],
   "source": [
    "head_view(attention, all_tokens, layer=0, heads=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4eb337-9184-4001-872e-21fdfce29c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
